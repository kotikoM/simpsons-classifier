{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simpsons Character Classification with CNN\n",
    "\n",
    "Implementation of a Convolutional Neural Network (CNN) from scratch to classify images of Simpsons characters.\n",
    "\n",
    "**Project Requirements:**\n",
    "- Train a CNN entirely from scratch (no pre-trained models allowed).\n",
    "- Use the hierarchical dataset provided in `characters_train/` where each folder corresponds to a character.\n",
    "- Ensure reproducibility by setting random seeds.\n",
    "- Save the trained model to disk after training.\n",
    "\n",
    "**Goals of this Notebook:**\n",
    "1. Load and preprocess the dataset.\n",
    "2. Build a CNN model from scratch.\n",
    "3. Train the model on the training set and validate its performance.\n",
    "4. Save the trained model for later inference."
   ],
   "id": "8277c5fbfc4e7f08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> **Important**\n",
    ">\n",
    "> This notebook does not work well with python version `3.14`. It was run on python `3.12`"
   ],
   "id": "44e7379eb2d7a786"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup Environment\n",
    "Import necessary libraries, configure device (CPU/GPU), and set random seeds for reproducibility."
   ],
   "id": "76698a7a7b84b605"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.452728600Z",
     "start_time": "2025-12-09T15:12:15.513588900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ],
   "id": "685c3067a48ed97d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.497254500Z",
     "start_time": "2025-12-09T15:12:17.453728300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "id": "e691c2b9aa747947",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.548088300Z",
     "start_time": "2025-12-09T15:12:17.499254600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "f0b9530ae9dbdfb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "- Dataset is located in `./data/simpsons/archive/characters_train/`.\n",
    "- Each folder corresponds to a character class.\n",
    "- Images will be resized to 64x64 pixels for uniformity. Reason for resizing will make sense later.\n",
    "- We'll map each class to a numeric label for training."
   ],
   "id": "684e90401fa64205"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.587965300Z",
     "start_time": "2025-12-09T15:12:17.550089600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"./data/simpsons/archive/characters_train\"\n",
    "pic_size = 64  # resize images to 64x64\n",
    "\n",
    "# Gather all class names\n",
    "class_names = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "idx_to_class = {i: name for name, i in class_to_idx.items()}"
   ],
   "id": "abba1f9602b1876b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.648645100Z",
     "start_time": "2025-12-09T15:12:17.592137600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of different characters: {len(class_names)}\")\n",
    "print(f\"Examples of names: {class_names[:5]}\")"
   ],
   "id": "df33d6cf6f3039d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters: 42\n",
      "Examples of names: ['abraham_grampa_simpson', 'agnes_skinner', 'apu_nahasapeemapetilon', 'barney_gumble', 'bart_simpson']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need numerical representation for each character name since CNNs don't work with string output. Fortunately number of characters is fixed number - 42.",
   "id": "fe97995ff78e888e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:17.691865400Z",
     "start_time": "2025-12-09T15:12:17.652821300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Class → Index mapping:\")\n",
    "print(list(class_to_idx.items())[:5])\n",
    "\n",
    "print(\"\\nIndex mapping -> Class:\")\n",
    "print(list(idx_to_class.items())[:5]) # good to have both mappings"
   ],
   "id": "320d719104b8e587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class → Index mapping:\n",
      "[('abraham_grampa_simpson', 0), ('agnes_skinner', 1), ('apu_nahasapeemapetilon', 2), ('barney_gumble', 3), ('bart_simpson', 4)]\n",
      "\n",
      "Index mapping -> Class:\n",
      "[(0, 'abraham_grampa_simpson'), (1, 'agnes_skinner'), (2, 'apu_nahasapeemapetilon'), (3, 'barney_gumble'), (4, 'bart_simpson')]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Exploring the Dataset\n",
    "\n",
    "- The dataset contains 42 characters and a total of 16,764 images. Each character has different number of pictures.\n",
    "- Each image is in RGB format, but they vary in dimensions and aspect ratios.\n",
    "- To feed the images into a CNN, we need to resize them to a uniform size. Common choices are 256×256, 128×128, or 64×64.\n",
    "- On my setup, resizing to 128×128 resulted in training times exceeding 10 minutes per epoch. Too long.\n",
    "- Resizing to 64×64 significantly reduced computation time while still producing reasonable accuracy, making it the practical choice for this project."
   ],
   "id": "dff03f44789bc73f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:18.104214700Z",
     "start_time": "2025-12-09T15:12:17.694415800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counts = {}\n",
    "\n",
    "for character in class_names:\n",
    "    character_dir = os.path.join(DATA_DIR, character)\n",
    "    num_images = sum(\n",
    "        1 for f in os.listdir(character_dir)\n",
    "         if os.path.isfile(os.path.join(character_dir, f)) and f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    )\n",
    "    counts[character] = num_images\n",
    "\n",
    "print(f\"Total number of classes: {len(class_to_idx)}\")\n",
    "print(f\"Total number of pictures: {sum(counts.values())}\")\n",
    "\n",
    "print(\"\\nImage counts per class:\")\n",
    "for name, count in counts.items():\n",
    "    print(f\"{name}: {count} images\")"
   ],
   "id": "dc4bd29e9c7905a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 42\n",
      "Total number of pictures: 16764\n",
      "\n",
      "Image counts per class:\n",
      "abraham_grampa_simpson: 731 images\n",
      "agnes_skinner: 34 images\n",
      "apu_nahasapeemapetilon: 499 images\n",
      "barney_gumble: 85 images\n",
      "bart_simpson: 1074 images\n",
      "carl_carlson: 79 images\n",
      "charles_montgomery_burns: 955 images\n",
      "chief_wiggum: 789 images\n",
      "cletus_spuckler: 38 images\n",
      "comic_book_guy: 376 images\n",
      "disco_stu: 7 images\n",
      "edna_krabappel: 366 images\n",
      "fat_tony: 22 images\n",
      "gil: 22 images\n",
      "groundskeeper_willie: 97 images\n",
      "homer_simpson: 1797 images\n",
      "kent_brockman: 399 images\n",
      "krusty_the_clown: 965 images\n",
      "lenny_leonard: 248 images\n",
      "lionel_hutz: 3 images\n",
      "lisa_simpson: 1084 images\n",
      "maggie_simpson: 103 images\n",
      "marge_simpson: 1033 images\n",
      "martin_prince: 57 images\n",
      "mayor_quimby: 197 images\n",
      "milhouse_van_houten: 864 images\n",
      "miss_hoover: 14 images\n",
      "moe_szyslak: 1162 images\n",
      "ned_flanders: 1164 images\n",
      "nelson_muntz: 287 images\n",
      "otto_mann: 26 images\n",
      "patty_bouvier: 58 images\n",
      "principal_skinner: 956 images\n",
      "professor_john_frink: 52 images\n",
      "rainier_wolfcastle: 36 images\n",
      "ralph_wiggum: 72 images\n",
      "selma_bouvier: 83 images\n",
      "sideshow_bob: 702 images\n",
      "sideshow_mel: 32 images\n",
      "snake_jailbird: 44 images\n",
      "troy_mcclure: 7 images\n",
      "waylon_smithers: 145 images\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Loading Images and Assigning Labels\n",
    "\n",
    "To train our CNN, we need to load all images from the dataset, resize them, and assign numeric labels for each class.\n",
    "\n",
    "We define a function `get_data_opencv_with_map()` that:\n",
    "\n",
    "1. Iterates through each character folder in the dataset.\n",
    "2. Loads all images using OpenCV.\n",
    "3. Converts them from BGR to RGB if needed.\n",
    "4. Resizes all images to a consistent size (`pic_size` × `pic_size`).\n",
    "5. Stores images and labels as NumPy arrays for easy processing."
   ],
   "id": "ef1a9d0d57c3f02b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:12:18.142092Z",
     "start_time": "2025-12-09T15:12:18.106218700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "def get_data_opencv_with_map(directory: str, BGR: bool = False):\n",
    "    \"\"\"\n",
    "    Load all images folder by folder using OpenCV and assign numeric labels.\n",
    "    Returns:\n",
    "        images: np.ndarray of shape (N, H, W, 3)\n",
    "        labels: np.ndarray of numeric labels (N)\n",
    "        num_classes: int\n",
    "        images_per_class: dict[int, list[np.ndarray]] mapping class index to list of images\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    images_per_class = {idx: [] for idx in range(len(class_names))}\n",
    "\n",
    "    for c in sorted(os.listdir(directory)):\n",
    "        folder_path = os.path.join(directory, c)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        class_id = class_to_idx[c]\n",
    "\n",
    "        for f in sorted(os.listdir(folder_path)):\n",
    "            fpath = os.path.join(folder_path, f)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(fpath)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                if BGR:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (pic_size, pic_size))\n",
    "                images.append(img)\n",
    "                labels.append(class_id)\n",
    "                images_per_class[class_id].append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {fpath}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels), len(class_names), images_per_class"
   ],
   "id": "793014e89d4998a1",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading and processing all images can take **some time**, especially for large datasets like this one (~16,764 images). Be patient while this cell runs. Once completed, the images are ready for training, and this step only needs to be done once per session.",
   "id": "865092152edf3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:15:07.398190900Z",
     "start_time": "2025-12-09T15:12:18.143607400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images, labels, num_classes, images_per_class = get_data_opencv_with_map(DATA_DIR, BGR=True)\n",
    "\n",
    "print(f\"Total images: {len(images)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ],
   "id": "723d735e3a32d10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 16764\n",
      "Number of classes: 42\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Creating Dataset and DataLoader\n",
    "\n",
    "We need a custom PyTorch dataset to feed our images and labels into the model. This dataset will:\n",
    "- Store the images and their corresponding labels.\n",
    "- Convert images to PyTorch tensors.\n",
    "- Normalize pixel values to [0, 1].\n",
    "- Support indexing for batching during training."
   ],
   "id": "7d62194212d9f3de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:22:31.855372700Z",
     "start_time": "2025-12-09T15:22:31.830482900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpsonsDataset(Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: numpy array of shape (N, H, W, 3)\n",
    "            labels: numpy array of numeric labels\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert image from HWC to CHW and normalize to [0,1]\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label"
   ],
   "id": "b6bbf4c58048999f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will use 85% of the data for training and 15% for validation.\n",
    "\n",
    "Stratified splitting ensures each character has the same proportion of images in both sets."
   ],
   "id": "1aca939a5f9f19b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:23:36.272799300Z",
     "start_time": "2025-12-09T15:23:29.766076700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split images and labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    images, labels, test_size=0.15, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpsonsDataset(X_train, y_train)\n",
    "val_dataset = SimpsonsDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ],
   "id": "c66dfaa88de13eb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 14249, Validation samples: 2515\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batching with DataLoader speeds up training and ensures that memory usage stays manageable. Shuffling the training data improves model generalization.",
   "id": "369953eb7a5c13a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Designing Model\n",
    "\n",
    "The architecture uses 4 convolutional layers, 2 max pooling layers, dropout for regularization, and fully connected layers to output predictions for 42 classes.\n",
    "\n",
    "Model is separated into separate file [model.py](model.py)."
   ],
   "id": "5bc68dc5fa15b729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:28:54.980571300Z",
     "start_time": "2025-12-09T15:28:54.941388800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN4Conv(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Convolutional layers ---\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   # Input: RGB image, Output: 32 feature maps, Kernel: 3x3\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)             # Input: 32 maps, Output: 32 maps, Kernel: 3x3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)  # Input: 32 maps, Output: 64 maps, Kernel: 3x3\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3)             # Input: 64 maps, Output: 64 maps, Kernel: 3x3\n",
    "\n",
    "        # --- Pooling layer ---\n",
    "        self.pool = nn.MaxPool2d(2)  # 2x2 Max Pooling reduces H and W by half\n",
    "\n",
    "        # --- Dropout layers ---\n",
    "        self.dropout25 = nn.Dropout(0.25)  # Regularization: randomly zero 25% of inputs\n",
    "        self.dropout50 = nn.Dropout(0.5)   # Regularization: randomly zero 50% of inputs\n",
    "\n",
    "        # --- Fully connected layers ---\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 512)  # Flattened conv output → 512 neurons\n",
    "        self.fc2 = nn.Linear(512, num_classes)   # Output layer → number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "        # Second convolutional block\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout50(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "id": "8f6727ef1de7a5e2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Design Reasoning:\n",
    "1. Four Convolutional Layers:\n",
    "    - The first two layers extract low-level features like edges, corners, and textures.\n",
    "    - The next two layers extract higher-level features, such as facial structures or hair patterns.\n",
    "    - After experimentation, 4 convolutional layers provided a good balance between accuracy and training time.\n",
    "2. Kernel Sizes and Padding:\n",
    "    - `3x3` kernels are standard for capturing local patterns while keeping computation reasonable.\n",
    "    - Padding is applied selectively to control feature map dimensions, ensuring proper flow into fully connected layers.\n",
    "3. Pooling Layers:\n",
    "    - `MaxPool2d(2)` halves the spatial size of the feature maps.\n",
    "    - This reduces computation and helps the model learn hierarchical features.\n",
    "4. Dropout Regularization:\n",
    "    - 25% dropout after conv blocks and 50% dropout after the first fully connected layer prevent overfitting.\n",
    "    - Dropout rates were tuned experimentally for the best validation performance.\n",
    "5. Fully Connected Layers:\n",
    "    - The first FC layer reduces the flattened feature maps to 512 neurons, allowing complex feature combinations.\n",
    "    - The second FC layer outputs logits for each of the 42 classes.\n",
    "6. ReLU Activation:\n",
    "    - ReLU introduces non-linearity, which helps the network learn complex relationships between pixels.\n",
    "\n",
    "---\n",
    "\n",
    "Justification of the Architecture\n",
    "- Tested multiple variations:\n",
    "    - 2–6 convolutional layers, different dropout rates, and different numbers of neurons in FC layers.\n",
    "- The current 4-conv + 512-FC design gave the best trade-off between training time and validation accuracy on this dataset.\n",
    "- Using larger input sizes (128x128) significantly increased computation time without improving accuracy much."
   ],
   "id": "bfef3d3fc3b5ed24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7. Training the Model\n",
    "\n",
    "Now that the dataset and model are ready, we train the CNN from scratch.\n",
    "\n",
    "We ensure reproducibility, use GPU if available, and log training/validation performance for each epoch."
   ],
   "id": "2364b75c5b6e8989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:42:13.141023600Z",
     "start_time": "2025-12-09T15:42:11.058614200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = CNN4Conv(num_classes).to(device)\n",
    "\n",
    "# Optimizer: Adam with small learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Loss function: CrossEntropy for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "fe6a2822722524c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training Function\n",
    "\n",
    "The `train_one_epoch` function runs a single pass over the training data.\n",
    "\n",
    "The model is in training mode, processes each batch, computes loss, backpropagates gradients, and updates weights.\n",
    "\n",
    "It returns the average loss and accuracy for the epoch."
   ],
   "id": "75937c0e10302cb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:45:51.879161200Z",
     "start_time": "2025-12-09T15:45:51.828422800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return avg_loss, accuracy"
   ],
   "id": "cb07033cf23b3843",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validation\n",
    "\n",
    "The `validate` function evaluates the model on the validation set in evaluation mode (dropout disabled).\n",
    "\n",
    "It computes loss and counts correct predictions without gradient calculations, providing average validation loss and accuracy to measure generalization."
   ],
   "id": "138a7f0cff8c3127"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T15:45:54.040917400Z",
     "start_time": "2025-12-09T15:45:53.994513800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on validation data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return avg_loss, accuracy"
   ],
   "id": "41b3a0bdfbaf75",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop\n",
   "id": "d47570d98bb3ab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:57:55.098728200Z",
     "start_time": "2025-12-09T15:50:16.476384400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} — \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "fb44749a3ba76c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 — Train Loss: 3.0310, Train Acc: 0.1432, Val Loss: 2.5844, Val Acc: 0.3137\n",
      "Epoch 2/40 — Train Loss: 2.3877, Train Acc: 0.3533, Val Loss: 2.1058, Val Acc: 0.4406\n",
      "Epoch 3/40 — Train Loss: 2.0817, Train Acc: 0.4419, Val Loss: 1.9019, Val Acc: 0.4998\n",
      "Epoch 4/40 — Train Loss: 1.8790, Train Acc: 0.4873, Val Loss: 1.7577, Val Acc: 0.5312\n",
      "Epoch 5/40 — Train Loss: 1.7001, Train Acc: 0.5290, Val Loss: 1.6079, Val Acc: 0.5698\n",
      "Epoch 6/40 — Train Loss: 1.5548, Train Acc: 0.5685, Val Loss: 1.4820, Val Acc: 0.6072\n",
      "Epoch 7/40 — Train Loss: 1.4136, Train Acc: 0.6076, Val Loss: 1.4057, Val Acc: 0.6330\n",
      "Epoch 8/40 — Train Loss: 1.2966, Train Acc: 0.6356, Val Loss: 1.3367, Val Acc: 0.6485\n",
      "Epoch 9/40 — Train Loss: 1.1822, Train Acc: 0.6661, Val Loss: 1.2711, Val Acc: 0.6668\n",
      "Epoch 10/40 — Train Loss: 1.0749, Train Acc: 0.6943, Val Loss: 1.2235, Val Acc: 0.6708\n",
      "Epoch 11/40 — Train Loss: 0.9953, Train Acc: 0.7139, Val Loss: 1.1715, Val Acc: 0.6954\n",
      "Epoch 12/40 — Train Loss: 0.9145, Train Acc: 0.7341, Val Loss: 1.1508, Val Acc: 0.7018\n",
      "Epoch 13/40 — Train Loss: 0.8112, Train Acc: 0.7610, Val Loss: 1.1189, Val Acc: 0.7157\n",
      "Epoch 14/40 — Train Loss: 0.7551, Train Acc: 0.7761, Val Loss: 1.0765, Val Acc: 0.7177\n",
      "Epoch 15/40 — Train Loss: 0.6851, Train Acc: 0.7937, Val Loss: 1.0998, Val Acc: 0.7229\n",
      "Epoch 16/40 — Train Loss: 0.6238, Train Acc: 0.8099, Val Loss: 1.1116, Val Acc: 0.7233\n",
      "Epoch 17/40 — Train Loss: 0.5717, Train Acc: 0.8244, Val Loss: 1.1068, Val Acc: 0.7288\n",
      "Epoch 18/40 — Train Loss: 0.5197, Train Acc: 0.8378, Val Loss: 1.0888, Val Acc: 0.7447\n",
      "Epoch 19/40 — Train Loss: 0.4824, Train Acc: 0.8510, Val Loss: 1.1274, Val Acc: 0.7400\n",
      "Epoch 20/40 — Train Loss: 0.4421, Train Acc: 0.8600, Val Loss: 1.1116, Val Acc: 0.7491\n",
      "Epoch 21/40 — Train Loss: 0.4082, Train Acc: 0.8680, Val Loss: 1.1130, Val Acc: 0.7507\n",
      "Epoch 22/40 — Train Loss: 0.3698, Train Acc: 0.8824, Val Loss: 1.1582, Val Acc: 0.7495\n",
      "Epoch 23/40 — Train Loss: 0.3491, Train Acc: 0.8852, Val Loss: 1.1567, Val Acc: 0.7551\n",
      "Epoch 24/40 — Train Loss: 0.3197, Train Acc: 0.8960, Val Loss: 1.1302, Val Acc: 0.7590\n",
      "Epoch 25/40 — Train Loss: 0.2960, Train Acc: 0.9037, Val Loss: 1.1829, Val Acc: 0.7590\n",
      "Epoch 26/40 — Train Loss: 0.2737, Train Acc: 0.9133, Val Loss: 1.1781, Val Acc: 0.7515\n",
      "Epoch 27/40 — Train Loss: 0.2679, Train Acc: 0.9145, Val Loss: 1.2307, Val Acc: 0.7571\n",
      "Epoch 28/40 — Train Loss: 0.2423, Train Acc: 0.9201, Val Loss: 1.2062, Val Acc: 0.7594\n",
      "Epoch 29/40 — Train Loss: 0.2306, Train Acc: 0.9245, Val Loss: 1.2449, Val Acc: 0.7646\n",
      "Epoch 30/40 — Train Loss: 0.2296, Train Acc: 0.9225, Val Loss: 1.2721, Val Acc: 0.7666\n",
      "Epoch 31/40 — Train Loss: 0.2106, Train Acc: 0.9309, Val Loss: 1.2451, Val Acc: 0.7622\n",
      "Epoch 32/40 — Train Loss: 0.2021, Train Acc: 0.9335, Val Loss: 1.2601, Val Acc: 0.7567\n",
      "Epoch 33/40 — Train Loss: 0.1888, Train Acc: 0.9375, Val Loss: 1.3016, Val Acc: 0.7646\n",
      "Epoch 34/40 — Train Loss: 0.1789, Train Acc: 0.9411, Val Loss: 1.3179, Val Acc: 0.7610\n",
      "Epoch 35/40 — Train Loss: 0.1801, Train Acc: 0.9399, Val Loss: 1.3186, Val Acc: 0.7618\n",
      "Epoch 36/40 — Train Loss: 0.1695, Train Acc: 0.9434, Val Loss: 1.2828, Val Acc: 0.7678\n",
      "Epoch 37/40 — Train Loss: 0.1574, Train Acc: 0.9498, Val Loss: 1.3398, Val Acc: 0.7630\n",
      "Epoch 38/40 — Train Loss: 0.1497, Train Acc: 0.9497, Val Loss: 1.3481, Val Acc: 0.7610\n",
      "Epoch 39/40 — Train Loss: 0.1486, Train Acc: 0.9521, Val Loss: 1.3942, Val Acc: 0.7646\n",
      "Epoch 40/40 — Train Loss: 0.1432, Train Acc: 0.9517, Val Loss: 1.3835, Val Acc: 0.7598\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It took me about 1 hour.\n",
    "\n",
    "**Note:** Training can take some time, especially on CPU.\n",
    "Using a GPU will speed up each epoch significantly.\n",
    "\n",
    "Across 40 epochs, training accuracy rose from about 14% to over 95%, while validation accuracy improved from around 31% to roughly 76%, pretty accurate. Loss steadily decreased, showing that the model learned meaningful features and improved its ability to classify the images."
   ],
   "id": "ac1747de0e3f3c01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Saving the Model\n",
    "These weights can be loaded later for inference without retraining, ensuring reproducibility."
   ],
   "id": "9e5dce3026e7df88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:02:13.437780800Z",
     "start_time": "2025-12-09T17:02:13.307986500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"simpsons_cnn4conv.pth\")\n",
    "print(\"Model weights saved to simpsons_cnn4conv.pth\")"
   ],
   "id": "ca29c67c20c69913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to simpsons_cnn4conv.pth\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For continuation see `inference.ipynb`\n",
   "id": "5ff6c2b8a67c80cd"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
